{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chapter 10\n",
    "\n",
    "### Exercise 1"
   ],
   "id": "829ecfb340ecd766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils import set_mode\n",
    "\n",
    "set_mode('local')"
   ],
   "id": "9bb241a442994148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "from dsets import LunaDataset\n",
    "\n",
    "def time_iterations(iter_count):\n",
    "    ds = LunaDataset()\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(iter_count):\n",
    "        var = ds[i]\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'{iter_count} iterations finished in {end - start} seconds.')\n",
    "\n",
    "def time_last_iterations(iter_count):\n",
    "    ds = LunaDataset()\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(iter_count):\n",
    "        var = ds[-i]\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'{iter_count} iterations finished in {end - start} seconds.')"
   ],
   "id": "3854d867ed9d01a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "time_iterations(1000)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "a) First run finished in 141 seconds.",
   "id": "b94b52c6ea58a5db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time_iterations(1000)",
   "id": "f69407f3d0a1dd6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "b) Second run finished in under 1 second.",
   "id": "d8c235c972b74872"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time_iterations(1000)",
   "id": "61884f6460927396",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "c) After clearing the cache, the runtime is back to 200 seconds.",
   "id": "4c458482c35c04ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_last_iterations(1000)\n",
    "time_last_iterations(1000)"
   ],
   "id": "751cdf8edb0571a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "d) Using the last 1000 samples has no impact on the runtime after being cached.",
   "id": "37a1754b25edbc2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exercise 2",
   "id": "415576fd7e470b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_iterations(1000)\n",
    "time_iterations(1000)"
   ],
   "id": "76c31c348518362e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After randomizing the list, both runs take quite a long time.",
   "id": "5a81b05a881f8bf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exercise 3",
   "id": "fb3d6087a759f92e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_iterations(1000)\n",
    "time_iterations(1000)"
   ],
   "id": "32ab0c8e08294fff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The getCt decorator does have an impact on the first loop. The second one remains the same, however.",
   "id": "c4e1b2fa9eff3240"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chapter 11\n",
    "\n",
    "### Exercise 1"
   ],
   "id": "cae87fd6c45d669b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils import set_mode\n",
    "\n",
    "set_mode('local')\n",
    "\n",
    "import time\n",
    "from dsets import LunaDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ],
   "id": "a04fe5c6eabdb9d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def time_dataloader(num_workers, batch_size=1):\n",
    "    # We only time the validation set, so it's faster\n",
    "    dataset = LunaDataset(\n",
    "        val_stride = 10,\n",
    "        is_val_set = True,\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size  = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory  = True\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for _ in tqdm(data_loader):\n",
    "        pass\n",
    "\n",
    "    print(f'Finished in {time.time() - start_time:.2f} seconds. Num_workers: {num_workers}.')"
   ],
   "id": "afb49aa25f491f56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time_dataloader(4)",
   "id": "ffbc85aaac1e573c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before cache is filled, iterating over the 55107 samples in the validation set takes 8 minutes and 10 seconds.",
   "id": "f59068a8e4b2a0aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time_dataloader(4)",
   "id": "3802cafccd7ae24d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The run time significantly reduces after the first epoch, as the data is already placed in the on disk cache now. The time to iterate over the validation set is now only 1 minute and 21 seconds.",
   "id": "569555bc2c7ec08e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for workers in range(1, 13):\n",
    "    time_dataloader(workers)"
   ],
   "id": "9dcf3710d36ae8c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "a) The number of workers has an impact on the runtime, though only a limited one: Initially, going from 1 worker to 2 workers reduced the runtime by 30 seconds, or almost a quarter of the initial runtime. The following modification of the workers does not have a visible effect anymore. The runtime stays constant after increasing the number further.",
   "id": "15aa250d3438e1be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time_dataloader(batch_size=1024, num_workers=12)",
   "id": "c3ba7df477bc254a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "c) The maximum combination fluctuates highly. I could not figure out what causes the problem, yet. When everything is fine, the maximum seems to be about `batch_size` 4 if `num_workers` is 12 and `batch_size` 512 if `num_workers` is 1.",
   "id": "3fd6c14bf557060c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 2\n",
    "\n",
    "There does not seem to be an observable difference."
   ],
   "id": "665b49d11c25c4fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chapter 12\n",
    "\n",
    "### Exercise 1"
   ],
   "id": "c4239d96143bf1e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "5fc75ce514e7d9e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# More general implementation of f_score: Recall is considered beta times as important as precision\n",
    "def f_score(preds, labels, beta=1, classification_threshold=0.5):\n",
    "    # True positives: Elements identified as nodules that are actually nodules\n",
    "    # False positives: Elements identified as nodules that are not nodules\n",
    "    # True negatives: Elements not identified as nodules that are not nodules\n",
    "    # False negatives: Elements not identified as nodules that are nodules\n",
    "\n",
    "    pos_label_mask = labels > classification_threshold  # Actual nodules\n",
    "    pos_pred_mask = preds > classification_threshold    # Elements identified as nodules\n",
    "\n",
    "    neg_label_mask = ~pos_label_mask    # Actual non-nodules\n",
    "    neg_pred_mask = ~pos_pred_mask      # Elements identifies as non-nodules\n",
    "\n",
    "    pos_count = int(pos_label_mask.sum())   # Number of actual nodules\n",
    "    neg_count = int(neg_label_mask.sum())   # Number of actual non-nodules\n",
    "\n",
    "    true_neg_count = int((neg_label_mask & neg_pred_mask).sum())    # Number of non-nodules identified as such\n",
    "    true_pos_count = int((pos_label_mask & pos_pred_mask).sum())    # Number of nodules identified as such\n",
    "\n",
    "    false_pos_count = neg_count - true_neg_count    # Num. of samples identified as nodules, even though they are not nodules\n",
    "    false_neg_count = pos_count - true_pos_count    # Num. of samples identified as non-nodules, even though they are nodules\n",
    "\n",
    "    pos_pred_count = np.float32(true_pos_count + false_pos_count)\n",
    "    precision = true_pos_count / pos_pred_count if pos_pred_count > 0 else 0\n",
    "\n",
    "    act_pos_count = np.float32(true_pos_count + false_neg_count)\n",
    "    recall = true_pos_count / act_pos_count if act_pos_count > 0 else 0\n",
    "\n",
    "    denominator = ((beta ** 2) * precision) + recall\n",
    "    return (1 + beta ** 2) * (precision * recall) / denominator if denominator > 0 else 0.0"
   ],
   "id": "263b639174508dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "b)\n",
    "To reiterate:\n",
    "\n",
    "- Recall: Number of samples correctly identified as positive against the number of actual positive samples\n",
    "- Precision: Number of samples correctly identified as positive against the number of samples identified as positive, whether wrong or right\n",
    "\n",
    "If we classify everything as positive, we have a lot of false positives, so precision will be very low. We will not have any false negatives, however, so recall will be high. If we classify everything as negative, both will be 0. In our case, we want to minimize false negatives, as we want to be really sure we miss no nodules, so we want to weigh recall higher than precision. In this case the F2 score is a better choice."
   ],
   "id": "3635793dddd1c78a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Finish other exercises",
   "id": "26744380094e4ddc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
